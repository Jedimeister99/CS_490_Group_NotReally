{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/vae-gaussian.png\"  width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# fashion_mnist = keras.datasets.fashion_mnist\n",
    "from matplotlib.markers import MarkerStyle \n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist \n",
    "from keras.layers import Lambda, Input, Dense \n",
    "from keras.losses import binary_crossentropy \n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import imageio\n",
    "from IPython.display import Image as Img\n",
    "os.chdir('c:/users/nicolas/documents/data/faces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to sort images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading all file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted_alphanumeric(glob(r'C:\\Users\\Nicolas\\Documents\\Data\\faces\\combined/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicolas\\\\Documents\\\\Data\\\\faces\\\\combined\\\\1_0_0_20161219140623097.jpg.chip.jpg',\n",
       " 'C:\\\\Users\\\\Nicolas\\\\Documents\\\\Data\\\\faces\\\\combined\\\\1_0_0_20161219140627985.jpg.chip.jpg',\n",
       " 'C:\\\\Users\\\\Nicolas\\\\Documents\\\\Data\\\\faces\\\\combined\\\\1_0_0_20161219140642920.jpg.chip.jpg',\n",
       " 'C:\\\\Users\\\\Nicolas\\\\Documents\\\\Data\\\\faces\\\\combined\\\\1_0_0_20161219154018476.jpg.chip.jpg',\n",
       " 'C:\\\\Users\\\\Nicolas\\\\Documents\\\\Data\\\\faces\\\\combined\\\\1_0_0_20161219154556757.jpg.chip.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping all correctly formatted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '1', '9', '_'], dtype='<U1'),\n",
       " array([12387, 11316,     1,     5], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([i[-34] for i in files], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping only men/women (not both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces = [i for i in files if (i[-34] == '1') and len(i[-37:-35].strip('\\\\').strip('d'))  == 2 ] # or in ('0', ''1'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = [i[-34] for i in files if (i[-34] == '1') and len(i[-37:-35].strip('\\\\').strip('d')) > 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_women():\n",
    "    faces = pd.read_csv('800_women.csv', header=None).values\n",
    "    faces = faces.ravel().tolist()\n",
    "    return faces\n",
    "faces = load_women()\n",
    "y = np.repeat(1, len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y) == len(faces), 'The X and Y are not of the same length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting shape info, making cropping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrow, ncol, nchan = 200, 200, 3\n",
    "nrow, ncol, nchan = 60, 60, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    if img.shape[0]<img.shape[1]:\n",
    "        x = img.shape[0]\n",
    "        y = img.shape[1]\n",
    "        crop_img = img[: , int(y/2-x/2):int(y/2+x/2)]\n",
    "    else:\n",
    "        x = img.shape[1]\n",
    "        y = img.shape[0]\n",
    "        crop_img = img[int(y/2-x/2):int(y/2+x/2) , :]\n",
    "\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and cropping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "x = []\n",
    "num_to_load = len(faces)\n",
    "for ix, file in enumerate(faces[:num_to_load]): \n",
    "    image = plt.imread(file, 'jpg')\n",
    "    image = Image.fromarray(image).resize((dim, dim)).convert('L')\n",
    "    image = crop(np.array(image))\n",
    "    x.append(image)\n",
    "print(f'{int(time() - start)} seconds')\n",
    "# y = y[:num_to_load]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turning the pictures into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy, xx = y.nbytes, x.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X is 11,520,000 bytes and the size of Y is 3,200 bytes.\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of X is {xx:,} bytes and the size of Y is {yy:,} bytes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "files, faces = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=2e-1)\n",
    "x, y = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training set is 640 and the size of the test set is 160.\n"
     ]
    }
   ],
   "source": [
    "trainsize, testsize = x_train.shape[0], x_test.shape[0]\n",
    "print(f'The size of the training set is {trainsize:,} and the '\\\n",
    "     f'size of the test set is {testsize:,}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling, casting the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "image_size = x_train.shape[1] * x_train.shape[1] \n",
    "x_train = np.reshape(x_train, [-1, image_size]) \n",
    "x_test = np.reshape(x_test, [-1, image_size]) \n",
    "x_train = x_train.astype('float32') / 255 \n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(intermediate_dim=512, latent_dim=2):    \n",
    "    \"\"\"    \n",
    "    Build VAE    \n",
    "    :param intermediate_dim: size of hidden layers of the encoder/decoder    \n",
    "    :param latent_dim: latent space size    \n",
    "    :returns tuple: the encoder, the decoder, and the full vae    \n",
    "    \"\"\"\n",
    "    \n",
    "    # encoder first    \n",
    "    \n",
    "    inputs = Input(shape=(image_size,), name='encoder_input')    \n",
    "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    \n",
    "    # latent mean and variance    \n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)    \n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    # reparameterization trick for random sampling    \n",
    "    # Note the use of the Lambda layer    \n",
    "    # At runtime, it will call the sampling function    \n",
    "    \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "                                                                \n",
    "    # full encoder encoder model    \n",
    "                                                                \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')    \n",
    "    encoder.summary()\n",
    "    \n",
    "    # decoder    \n",
    "                                                                \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')    \n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)    \n",
    "    outputs = Dense(image_size, activation='sigmoid')(x)\n",
    "    \n",
    "    # full decoder model  \n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')    \n",
    "    decoder.summary()\n",
    "\n",
    "    # VAE model    \n",
    "\n",
    "    outputs = decoder(encoder(inputs)[2])    \n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    # Loss function    \n",
    "    # we start with the reconstruction loss    \n",
    "\n",
    "    reconstruction_loss = binary_crossentropy(inputs, outputs) * image_size\n",
    "\n",
    "    # next is the KL divergence\n",
    "\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)    \n",
    "    kl_loss = K.sum(kl_loss, axis=-1)    \n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    # we combine them in a total loss    \n",
    "    \n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    vae.add_loss(vae_loss)\n",
    "    \n",
    "    return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making a sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args: tuple):    \n",
    "    \"\"\"    \n",
    "    Reparameterization trick by sampling z from unit Gaussian    \n",
    "    :param args: (tensor, tensor) mean and log of variance of q(z|x)    \n",
    "    :returns tensor: sampled latent vector z    \n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack the input tuple    \n",
    "    \n",
    "    z_mean, z_log_var = args\n",
    "    \n",
    "    # mini-batch size    \n",
    "    \n",
    "    mb_size = K.shape(z_mean)[0]\n",
    "    \n",
    "    # latent space size    \n",
    "    \n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    # random normal vector with mean=0 and std=1.0    \n",
    "    \n",
    "    epsilon = K.random_normal(shape=(mb_size, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_distribution(encoder, x_test, y_test, batch_size=128):    \n",
    "    \"\"\"    \n",
    "    Display a 2D plot of the digit classes in the latent space.\n",
    "    We are interested only in z, so we only need the encoder here.    \n",
    "    :param encoder: the encoder network    \n",
    "    :param x_test: test images    \n",
    "    :param y_test: test labels    \n",
    "    :param batch_size: size of the mini-batch    \n",
    "    \"\"\"    \n",
    "    \n",
    "    z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    markers = ('o', 'x', '^', '<', '>', '*', 'h', 'H', 'D', 'd', 'P', 'X', '8', 's', 'p')\n",
    "    \n",
    "    for i in np.unique(y_test):\n",
    "        plt.scatter(z_mean[y_test == i, 0], z_mean[y_test == i, 1],\n",
    "        marker=MarkerStyle(markers[int(i)], fillstyle='none'), edgecolors='black')\n",
    "    \n",
    "    plt.xlabel(\"z[0]\")    \n",
    "    plt.ylabel(\"z[1]\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(decoder):    \n",
    "    \"\"\"    \n",
    "    Display a 2D plot of the generated images.    \n",
    "    We only need the decoder, because we'll manually sample the distribution z    \n",
    "    :param decoder: the decoder network    \n",
    "    \"\"\"\n",
    "    \n",
    "    # display a nxn 2D manifold of digits    \n",
    "    \n",
    "    n = 4 # used to be 15    \n",
    "    digit_size = nrow # used to be 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))    \n",
    "    \n",
    "    # linearly spaced coordinates corresponding to the 2D plot    \n",
    "    # of digit classes in the latent space    \n",
    "    \n",
    "    low = -1\n",
    "    high = 1\n",
    "    \n",
    "    grid_x = np.linspace(low, high, n)    \n",
    "    grid_y = np.linspace(low, high, n)[::-1]\n",
    "    \n",
    "    # start sampling z1 and z2 in the ranges grid_x and grid_y    \n",
    "    \n",
    "    for i, yi in enumerate(grid_y):        \n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])            \n",
    "            x_decoded = decoder.predict(z_sample)            \n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)             \n",
    "            slice_i = slice(i * digit_size, (i + 1) * digit_size)            \n",
    "            slice_j = slice(j * digit_size, (j + 1) * digit_size)            \n",
    "            figure[slice_i, slice_j] = digit\n",
    "            \n",
    "     # plot the results    \n",
    "    \n",
    "    plt.figure(figsize=(6, 5)) # was 6, 5    \n",
    "    start_range = digit_size // 2    \n",
    "    end_range = n * digit_size + start_range + 1    \n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)    \n",
    "    sample_range_x = np.round(grid_x, 1)    \n",
    "    sample_range_y = np.round(grid_y, 1)    \n",
    "    plt.xticks(pixel_range, sample_range_x)    \n",
    "    plt.yticks(pixel_range, sample_range_y)    \n",
    "    plt.xlabel(\"z[0]\")    \n",
    "    plt.ylabel(\"z[1]\")    \n",
    "    plt.imshow(figure, cmap='Greys_r')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_s = EarlyStopping(monitor='val_loss', patience=100)\n",
    "m_c = ModelCheckpoint('vae_weights.hdf5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the entire thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 3600)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1843712     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,845,764\n",
      "Trainable params: 1,845,764\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3600)              1846800   \n",
      "=================================================================\n",
      "Total params: 1,848,336\n",
      "Trainable params: 1,848,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Nicolas\\Anaconda3\\envs\\Pytorch Env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 1845764   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 3600)              1848336   \n",
      "=================================================================\n",
      "Total params: 3,694,100\n",
      "Trainable params: 3,694,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\Anaconda3\\envs\\Pytorch Env\\lib\\site-packages\\keras\\engine\\training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    encoder, decoder, vae = build_vae()\n",
    "    vae.compile(optimizer=Adam(lr=0.002)) \n",
    "    vae.summary()\n",
    "\n",
    "    \n",
    "#     vae.fit(x_train, epochs=500, batch_size=16, # was \n",
    "#             validation_data=(x_test, None),\n",
    "#             callbacks=[e_s, m_c],\n",
    "#             verbose=1)\n",
    "    \n",
    "#     plot_generated_images(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('vae_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1528a225fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(vae.predict(np.random.uniform(-1, 1, dim*dim).reshape(1, \n",
    "                            -1)).reshape(dim, dim), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = np.linspace(0.1, .7, n)\n",
    "max_ = np.minimum(min_ + np.random.uniform(0.01, 0.02, n), .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(n, minimum, maximum):\n",
    "        face = []\n",
    "        for i in range(n):\n",
    "            face.append(vae.predict(np.random.uniform(minimum, maximum, dim*dim).reshape(1, -1)).reshape(dim, dim))\n",
    "        return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('vae_800_women'):\n",
    "    os.mkdir('vae_800_women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# png_dir = 'vae_800_women'\n",
    "# images = []\n",
    "# for file_name in sorted_alphanumeric(os.listdir(png_dir))[:-20]:\n",
    "#     if file_name.endswith('.jpg'):\n",
    "#         file_path = os.path.join(png_dir, file_name)\n",
    "#         images.append(imageio.imread(file_path))\n",
    "# images.extend(images[::-1])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  completed.\n",
      "2000  completed.\n",
      "3000  completed.\n",
      "4000  completed.\n",
      "5000  completed.\n",
      "6000  completed.\n",
      "7000  completed.\n",
      "8000  completed.\n",
      "9000  completed.\n",
      "10000  completed.\n",
      "11000  completed.\n",
      "12000  completed.\n",
      "13000  completed.\n",
      "14000  completed.\n",
      "15000  completed.\n",
      "16000  completed.\n",
      "17000  completed.\n",
      "18000  completed.\n",
      "19000  completed.\n",
      "20000  completed.\n",
      "21000  completed.\n",
      "22000  completed.\n",
      "23000  completed.\n",
      "24000  completed.\n",
      "25000  completed.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "n = 25_000\n",
    "for i in range(1, n + 1):\n",
    "    pic = np.mean(gen_samples(n=50, minimum=np.random.uniform(0, 1), \n",
    "                              maximum=np.random.uniform(0, 1)), axis=0)*255\n",
    "    filename = 'vae_800_women/{}.jpg'.format(str(i))\n",
    "    im = Image.fromarray(pic.astype(np.uint8))\n",
    "    im.save(filename)\n",
    "    if (i+1) % 1_000 == 0:\n",
    "        print(i+1, ' completed.')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageio.mimsave('c:/users/nicolas/desktop/sickgif.gif', images, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Img(filename='c:/users/nicolas/desktop/sickgif.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_generated_images(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('c:/users/nicolas/desktop.genfaces.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
