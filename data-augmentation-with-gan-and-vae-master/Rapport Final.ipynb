{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN-Generated Samples to Offset Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vincent Fortin (11249631) | \n",
    "Nicolas Gervais (11263889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Class Imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with imbalanced classes represents a challenge that most machine learning practitioners will face. Indeed, many learning algorithms are suited for balanced datasets, such as Support Vector Machines (SVM), decision trees, and logistic regression [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0020025513005124). When combined with a limited number of training instances, imbalanced classes can result in poorly trained models. Having few instances from which to learn, algorithms may have a limited ability to generalize, and therefore suffer from poor performance on unseen data. These problems have most frequently sparked research in the fields of neurocomputing, knowledge-based systems, but also in image recognition [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub). Many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various strategies have been suggested to negate the effects of class imbalance, which typically fall into three categories, oversampling, undersampling, and hybrid methods [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the category of oversampling, probably the most popular strategy is to use the Synthetic Minority Over-sampling Technique (SMOTE). As its name suggests, SMOTE is an oversampling method, which works by creating synthetic samples from the minor class instead of creating copies [$^{ref}$](https://jair.org/index.php/jair/article/view/10302). The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. Many rules have been put forward to weigh minority instances differently. A suggestion has been to cluster minority instances using a semi-unsupervised hierarchical clustering approach to determine the size to oversample each sub-cluster using its classification complexity and cross-validation. Then, the minority instances are oversampled depending on their Euclidean distance to the majority class. [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417415007356) <font color='red'>_this excerpt was copy pasted_</font>. Minority instances can also be weighted according to their distance to the majority class [$^{ref}$](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180830). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar family of strategies is the undersampling of the majority class. Interestingly, the most effective method of this kind is to delete random samples until the size of the majority and minority classes match [$^{ref}$](https://link.springer.com/chapter/10.1007%2F978-3-642-02326-2_9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, hybrid methods are a mixture of the two aforementioned strategies. A recent meta-analysis from Haixiang and colleagues (2017) offers more details of the latest developments of research on imbalanced datasets [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 What has been done in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counteracting the effects of class imbalance in image recognition tasks add another layer of difficulty. Yet, some methods have been suggested. By matching pairs of images (taking the mean of every pixel), accuracy was improved on the CIFAR-10, compared to the generic dataset [$^{ref}$](https://arxiv.org/abs/1801.02929). In a similar fashion, state-of-the-art results on the CIFAR-10 and ImageNet have been achieved using translation, rotation, or shearing of different magnitudes [$^{ref}$](https://arxiv.org/abs/1805.09501). Another method to provide more training samples was to cut the \"main\" component of the image, and paste it on different backgrounds [$^{ref}$](http://openaccess.thecvf.com/content_iccv_2017/html/Dwibedi_Cut_Paste_and_ICCV_2017_paper.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possibilities to generate more training samples include generative adversarial networks (GAN) and variational auto-encoders (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are neural networks defined by a _generator_ and a _discriminator_. The former generates increasingly realistic samples, and the discriminator determines if the samples looks \"real\" or \"fake\". The term adversarial refers to the competitive nature of the interaction between generation and discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 How GAN has been used to counter class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers have used GANs to generate new minority samples [$^{ref}$](https://arxiv.org/abs/1807.04585). A balanced GAN (BAGAN) was designed with both the majority and minority class to learn useful features. The authors found that the pictures generated were of higher quality than simply using the minority class [$^{ref}$](https://arxiv.org/abs/1803.09655). However, the authors did not test if this resulted in a more accurate classifier. Similar to our research question, researchers have used GANs to generate instances of multiple classes, and found increased CNN accuracy [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0925231219309257?dgcid=rss_sd_all), over and above generic oversampling. Importantly, intra-class heterogeneity must be captured by the GAN, to provide new boundaries to the parameter space. With this concern in mind, Huang and colleagues (2019) improved classification accuracy with their actor-critic GAN (AC-GAN) [$^{ref}$](https://ieeexplore.ieee.org/document/8784774)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Our experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of GANs and VAEs, we will generate a multitude of samples for a minority class, and determine if these generated samples improve a classifier. A simple face recognition task will be assessed: to determine the sex of the person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used will be the UTK Face Dataset [$^{ref}$](http://aicip.eecs.utk.edu/wiki/UTKFace), which consists of over 20,000 face images with annotations of age, gender, and ethnicity. Only the pictures labeled as between 20 and 100 years old will be included. 8,000 samples will be kept for both the male and female categories. Next, the female class will be reduced to 10% of its original size, in order to weaken the classifier. The picture size is 60x60 in grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Our benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we downsample women until we get 70% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Our metrics / CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier will be a convolutional neural network (CNN) <font color='orange'>(describe shortly)</font>. CNNs are neural networks with at least one convolutional layer, which serves as a feature detector. See the following figure for the exact architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here plot keras pydot model plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier will be trained with both the _complete_ classes, and this performance will be established as the original baseline benchmark. Additionally, the CNN will be trained with the reduced female class, and it will be set as the lower bound classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the experiment will undergo various levels of imbalance, classification performance will be measured with the area under the curve (AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, five models will be used to generate samples. All models contain interesting particularities for the task at hand, which are promising in unique ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1 Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational auto-encoder (VAE) consists of an encoder, a decoder, and a loss function. The encoder transforms its input in a hidden representation, also called its latent representation space. This space is much less than the input dimensions. This is typically referred to as a ‘bottleneck’ because the encoder must learn an efficient compression of the data into this lower-dimensional space. The lower-dimensional space is stochastic: the encoder outputs parameters as a Gaussian probability density. We can sample from this distribution to get noisy values of the representations zz. <font color='red'> _this is copy pasted_ </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is another neural net. Its input is the representation output by the encoder, it outputs the parameters to the probability distribution of the data. By re-expanding the representation, we can determine how much information is lost with the reconstruction log-likelihood loss. This measure tells us how effectively the decoder has learned to reconstruct an input image xx given its latent representation zz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function of the variational autoencoder is the negative log-likelihood with a regularizer. If the decoder’s output does not reconstruct the data well, statistically we say that the decoder parameterizes a likelihood distribution that does not place much probability mass on the true data. The second term is a regularizer that we throw in (we’ll see how it’s derived later). This is the Kullback-Leibler divergence.  between the encoder’s distribution q_\\theta(z\\mid x)q\n",
    "​θ\n",
    "​​ (z∣x) and p(z)p(z). This divergence measures how much information is lost (in units of nats) when using qq to represent pp. It is one measure of how close qq is to pp.\n",
    "\n",
    "In the variational autoencoder, pp is specified as a standard Normal distribution with mean zero and variance one, or p(z) = Normal(0,1)p(z)=Normal(0,1). If the encoder outputs representations zz that are different than those from a standard normal distribution, it will receive a penalty in the loss. This regularizer term means ‘keep the representations zz of each digit sufficiently diverse’. If we didn’t include the regularizer, the encoder could learn to cheat and give each datapoint a representation in a different region of Euclidean space. This is bad, because then two images of the same number (say a 2 written by different people, 2_{alice}2\n",
    "​alice\n",
    "​​  and 2_{bob}2\n",
    "​bob\n",
    "​​ ) could end up with very different representations z_{alice}, z_{bob}z\n",
    "​alice\n",
    "​​ ,z\n",
    "​bob\n",
    "​​ . We want the representation space of zz to be meaningful, so we penalize this behavior. This has the effect of keeping similar numbers’ representations close together (e.g. so the representations of the digit two {z_{alice}, z_{bob}, z_{ali}}z\n",
    "​alice\n",
    "​​ ,z\n",
    "​bob\n",
    "​​ ,z\n",
    "​ali\n",
    "​​  remain sufficiently close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not copy pasted\n",
    "\n",
    "The variational auto-encoder is an auto-encoder which is used mostly to generate data. It's structure is similar to regular auto-encoders, where we try to find a lower dimentionentional representation of the data by creating a bottleneck in the middle of the neural networks (encoder and decoder).\n",
    "The reason we can't generate examples from regular auto encoder is that we don't know what the distribution of the hidden bottleneck layer, meaning that if we were to pass random values through the decoder, we would most likely get a reconstruction that would look nothing like our other examples.\n",
    "\n",
    "In order to know what kind of inputs passed to the decoder will reproduce examples which look like the population examples, we need to make changes to the hidden bottleneck layer. We need to change it so that we can sample from a distribution in the hidden layer, instead of imputting random fixed values to the decoder.\n",
    "In order to efficiently train the autoencoder with backpropagation, slight changes need to be made to the hidden bottleneck, since backpropagation can't be calculated on a distribution.\n",
    "TALK ABOUT CHANGES ?\n",
    "\n",
    "Once the model is trained, we will have learned the latent distribution of the bottleneck layer and we can sample points from this distribution. Sampling a point from the latent distribution, and passing it trough the decoder, we can generate a new unique example. This new example will be based on other examples which were used to train the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 Adversarial Auto-Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Generative adversarial networks (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are comprised of two neural networks, namely a _generator_ and a _discriminator_.\n",
    "\n",
    "The goal of the former is to generate increasingly realistic samples, while the latter, the discriminator's goal is to determine if the samples are \"real\" or \"fake\". The term adversarial refers to the competitive nature of the interaction between the generator and discriminator. \n",
    "\n",
    "Here is how this works in more details.\n",
    "As we said GANs are composed of 2 competing neural networks. \n",
    "The first part is the discriminator, which is a neural network trained to differentiate between real and fake examples, in our case images. The way it is trained at first is to feed it real examples, as well as random noise. In later phases, the discriminator is trained on examples that were generated by the generator, as well as real examples from the original dataset.\n",
    "\n",
    "The Generator's goal is to generate more and more realistic fake examples. When we talk about realistic examples, we mean that the fake examples generated are classified as real by the discriminator.\n",
    "To generate examples, the network takes in a random point in a input space, called latent space, passes it through the neural network and outputs an image. At first, the network will output random pixels since it doesn't have a good way to map the points in the latent space to images.\n",
    "To train the generator, we need to find a way to connect the discriminator's loss function to the generator weights, meaning that when the generator generates a __poor quality image__, we need to use the information from the discriminator (how it figured out that it was a fake) to update the weights from the generator.\n",
    "\n",
    "This can be done with backpropagation __(explain if needed)__.\n",
    "\n",
    "Both networks are trained asynchronously and the training stops when both loss functions stabilyze. \n",
    "\n",
    "The steps to training the whole network are as followed:\n",
    "1- Train the discriminator: Generate one (or few) images from random noise (random point in latent space). Since the generator has not been trained yet, the images will be of poor quality. Use those images and the same amount of real images to train the discriminator classifier (real vs fake).\n",
    "2- Train the generator: Generate one (or few) images from a point in the latent space. Pass them through the discriminator classifier and use the loss function (how good our fake images are) to update the weights of the generator.\n",
    "3- Repeat steps 1 and 2 until both loss functions converge. The goal here is to have the generator's loss function be very low, while having the discriminator's loss function be high (accuracy at 0.5).\n",
    "\n",
    "We can think of it as a min max game, where the discriminator tries to minimize a loss function d(z), where z is the latent space and where the generator tries to maximize a d(g(z)).\n",
    "\n",
    "In the original GAN paper, the authors used a mixture of rectifier linear activations and sigmoid activations for the discriminator, and maxout activation for the discriminator. They also noted that it is technically possible to use any differentiable activation function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Problems with GANS:\n",
    "Mode collapse: When we generate new data points, we would like them to be close enough from real examples, but with enough variety so that examples are not exact copies of the training examples, but also that all of the classes are represented, with similar proportions as the training set.\n",
    "The problem of classes being over/under represented in the learned distribution by the generator, versus the real (training) distribution is known as mode collapse. This problem arises because of the fact that the generator is trying to 'trick' the discriminator by exploiting it's weaknesses. What can happen concretly is that the generator generates images from a single class it 'thinks' the discriminator has the biggest difficulty classifying as real/fake. When the discriminator has learned to properly classify those examples from the single class, the generator will move to another class entirely. This process will happen 'forever' and the generator will never learn the proper distribution of the data. \n",
    "Many variations to the GAN try to deal with this problem, notably one which changes the amount of examples provided to the discriminator from 1 to a small amount. The goal of the generator is thus to predict if the batch of data points is real or fake as a whole. If the generator tries to generate a batch of data points which all come from the same class, the discriminator will be able to figure out that those points do not resemble the training distribution.\n",
    "Changes in the loss functions can mitigate this problem and we will further discuss them later.\n",
    "\n",
    "Diminished gradient: \n",
    "This problem also comes from the fact that we have two competing neural networks. The diminished gradient problem comes in when the discriminator is very good and it's performance is very high. Since we perform gradient descent based on the discriminator's loss function, when the discriminator's performance is very high, it's loss function is very low and so is it's gradient. When we perform gradient descent to optimize neural networks or other learning algorithms, small gradients usually mean we are close to the optimum and we can stop the optimisation. In the case of GANs, this only means that the discriminator is close to it's optimum and says little about the generator, which is what we are trying to use.\n",
    "Since we spread the gradient from the discriminator to the generator to optimize the generator's neural network, when the gradient used for the backpropagation is very small, the weights of the network are left almost unchanged, which means that the generator stops learning.\n",
    "\n",
    "To mitigate this problem, ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Softmax GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its success in many applications, GAN is highly unstable in training. Careful selection of\n",
    "hyperparameters is often necessary to make the training process converge [11]. It is often believed\n",
    "that this instability is caused by unbalanced discriminator and generator training. As the discriminator\n",
    "utilizes a logistic loss, it saturates quickly and its gradient vanishes if the generated samples are easy\n",
    "to separate from the real ones. When the discriminator fails to provide gradient, the generator stops\n",
    "updating. Softmax GAN overcomes this problem by utilizing the softmax cross-entropy loss, whose\n",
    "gradient is always non-zero unless the softmaxed distribution matches the target distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.4 Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5 Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Our results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy on AAE : 80.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Alternative results (CNN trained on original data, test is generated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
