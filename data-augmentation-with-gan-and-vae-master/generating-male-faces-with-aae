import numpy as np
import matplotlib.pyplot as plt
from glob import glob
from PIL import Image
from time import time
import re
import pandas as pd
import os
import argparse
import math
import itertools
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch

os.chdir('C:/Users/Nicolas/Documents/Data/Faces')

files = glob('combined/*.jpg')

faces = [i for i in files if (i[-34] == '0') and len(i[-37:-35].strip('\\').strip('d')) == 2]
y = [i[-34] for i in files if (i[-34] == '0') and len(i[-37:-35].strip('\\').strip('d')) > 1]

dim = 60

start = time()
x = list()
num_to_load = len(faces)  
for ix, file in enumerate(faces[:num_to_load]):
    image = plt.imread(file, 'jpg')
    if image.shape[0] != image.shape[1]:
        prob += 1
    image = Image.fromarray(image).resize((dim, dim)).convert('L')
    image = np.array(image)
    x.append(image)

x = np.array(x, dtype=np.float32).reshape(-1, 1, 60, 60)

assert x.ndim == 4, 'The input is the wrong shape!'

files, faces = None, None

x = x.astype(np.float32) / 127.5 - 1
y = np.array(y, dtype=np.float32)

if torch.cuda.is_available():
    x = torch.from_numpy(x) 
    y = torch.from_numpy(y)
    print('Tensors successfully flushed to CUDA.')
else:
    print('CUDA not available!')


class Face:
    
    def __init__(self):
        self.len = x.shape[0]
        self.x = x
        self.y = y
        
    def __getitem__(self, index):
        return x[index], y[index].unsqueeze(0)
    
    def __len__(self):
        return self.len


train = Face()

parser = argparse.ArgumentParser()

parser.add_argument("--n_epochs", type=int, default=100, help="number of epochs of training")
parser.add_argument("--batch_size", type=int, default=32, help="size of the batches")
parser.add_argument("--lr", type=float, default=0.005, help="adam: learning rate")
parser.add_argument("--b1", type=float, default=0.3, help="adam: decay of first order momentum of gradient")
parser.add_argument("--b2", type=float, default=0.999, help="adam: decay of first order momentum of gradient")
parser.add_argument("--n_cpu", type=int, default=8, help="number of cpu threads to use during batch generation")
parser.add_argument("--latent_dim", type=int, default=3, help="dimensionality of the latent code")
parser.add_argument("--img_size", type=int, default=60, help="size of each image dimension")
parser.add_argument("--channels", type=int, default=1, help="number of image channels")
parser.add_argument("--sample_interval", type=int, default=50, help="interval between image sampling")
opt, unknown = parser.parse_known_args()

img_shape = (opt.channels, opt.img_size, opt.img_size)

cuda = True if torch.cuda.is_available() else False


def reparameterization(mu, logvar):
    std = torch.exp(logvar / 2)
    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))
    z = sampled_z * std + mu
    return z


class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.mu = nn.Linear(512, opt.latent_dim)
        self.logvar = nn.Linear(512, opt.latent_dim)

    def forward(self, img):
        img_flat = img.view(img.shape[0], -1)
        x = self.model(img_flat)
        mu = self.mu(x)
        logvar = self.logvar(x)
        z = reparameterization(mu, logvar)
        return z


class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(opt.latent_dim, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, int(np.prod(img_shape))),
            nn.Tanh(),
        )

    def forward(self, z):
        img_flat = self.model(z)
        img = img_flat.view(img_flat.shape[0], *img_shape)
        return img


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(opt.latent_dim, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )

    def forward(self, z):
        validity = self.model(z)
        return validity


adversarial_loss = torch.nn.BCELoss()
pixelwise_loss = torch.nn.L1Loss()

encoder = Encoder()
decoder = Decoder()
discriminator = Discriminator()

decoder.load_state_dict(torch.load('aae_decoder_men'))
encoder.load_state_dict(torch.load('aae_encoder_men'))
discriminator.load_state_dict(torch.load('aae_discriminator_men'))

if cuda:
    encoder.cuda()
    decoder.cuda()
    discriminator.cuda()
    adversarial_loss.cuda()
    pixelwise_loss.cuda()

dataloader = torch.utils.data.DataLoader(train, batch_size=opt.batch_size, shuffle=True)

optimizer_G = torch.optim.Adam(
    itertools.chain(encoder.parameters(),
    decoder.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2))

optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))

Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor


def sample_image(n_row, batches_done, directory):
    """Saves a grid of generated digits"""
    # Sample noise
    z = Variable(Tensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))
    gen_imgs = decoder(z)
    save_image(gen_imgs.data, "%s/%d.png" % (directory, batches_done), nrow=n_row, normalize=True)


if not os.path.isdir('generated_men_aae'):
            os.mkdir('generated_men_aae')
        
for epoch in range(1, opt.n_epochs + 1):

    break # model already trained
    for i, (imgs, _) in enumerate(dataloader):

        # Adversarial ground truths
        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)
        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------

        optimizer_G.zero_grad()

        encoded_imgs = encoder(real_imgs)
        decoded_imgs = decoder(encoded_imgs)

        # Loss measures generator's ability to fool the discriminator
        g_loss = 0.001 * adversarial_loss(discriminator(encoded_imgs), valid) + 0.999 * pixelwise_loss(
            decoded_imgs, real_imgs
        )

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_D.zero_grad()

        # Sample noise as discriminator ground truth
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(discriminator(z), valid)
        fake_loss = adversarial_loss(discriminator(encoded_imgs.detach()), fake)
        d_loss = 0.5 * (real_loss + fake_loss)

        d_loss.backward()
        optimizer_D.step()
        
    batches_done = epoch * len(dataloader) + i

    if epoch >= 25 and epoch % 10 == 0:
        val = input("\nContinue training? [y/n]: ")
        print()
        if val in ('y', 'yes'):
            val = True
            pass
        elif val in ('n', 'no'):
            break  
        else:
            pass

    if epoch > 10:
        if batches_done % opt.sample_interval == 0:
            sample_image(n_row=5, batches_done=batches_done, directory='generated_men_aae')

    if epoch % 5 == 0:
        print(
            "[Epoch %d/%d] [D loss: %f] [G loss: %f]"
            % (epoch, opt.n_epochs, d_loss.item(), g_loss.item())
        )        

torch.save(decoder.state_dict(), 'aae_decoder_men')
torch.save(encoder.state_dict(), 'aae_encoder_men')
torch.save(discriminator.state_dict(), 'aae_discriminator_men')

images = 0
stop = False
for epoch in range(1, 4 + 1):
    for i, (imgs, _) in enumerate(dataloader):
        
        with torch.no_grad():

            # Adversarial ground truths
            valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)
            fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)

            # Configure input
            real_imgs = Variable(imgs.type(Tensor))

            batches_done = epoch * len(dataloader) + i
            sample_image(directory='generated_men_aae', n_row=5, batches_done=batches_done)
            images += 25

            if len(os.listdir(os.path.join(os.getcwd(), 'generated_men_aae'))) >= 1000:
                stop = True
                break

    if stop:
        break

