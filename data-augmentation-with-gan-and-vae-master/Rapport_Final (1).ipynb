{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GAN-Generated Samples to Offset Class Imbalance](https://github.com/nicolas-gervais/data-augmentation-with-gan-and-vae) {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vincent Fortin (11249631) | \n",
    "Nicolas Gervais (11263889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction {-}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with imbalanced classes represents a challenge that most machine learning practitioners will face. Indeed, many learning algorithms are suited for balanced datasets, such as Support Vector Machines (SVM), decision trees, and logistic regression [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0020025513005124). When combined with a limited number of training instances, imbalanced classes can result in poorly trained models. Having few instances from which to learn, algorithms may have a limited ability to generalize, and therefore suffer from poor performance on unseen data. These problems have most frequently sparked research in the fields of neurocomputing, knowledge-based systems, but also in image recognition [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Working with class imbalance \\newline \\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various strategies have been suggested to negate the effects of class imbalance, which typically fall into three categories, oversampling, undersampling, and hybrid methods [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the category of oversampling, probably the most popular strategy is to use the Synthetic Minority Over-sampling Technique (SMOTE). As its name suggests, SMOTE is an oversampling method, which works by creating synthetic samples from the minor class instead of creating copies [$^{ref}$](https://jair.org/index.php/jair/article/view/10302). The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. Many rules have been put forward to weigh minority instances differently. For example, minority instances could be weighted based on their distance from the majority class instances[$^{ref}$](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180830)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar family of strategies is the undersampling of the majority class. Interestingly, the most effective method of this kind is to delete random samples until the size of the majority and minority classes match [$^{ref}$](https://link.springer.com/chapter/10.1007%2F978-3-642-02326-2_9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, hybrid methods are a mixture of the two aforementioned strategies. A recent meta-analysis from Haixiang and colleagues (2017) offers more details of the latest developments of research on imbalanced datasets [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub).\\newline\\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What has been done in machine learning \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counteracting the effects of class imbalance in image recognition tasks add another layer of difficulty. This is because of the fact that we are working in high dimensional data, and that pixels close to each others are correlated. Using traditional distance measures in this context would not yield very good results.<br>\n",
    "This being said, some methods have been suggested. By matching pairs of images (taking the mean of every pixel), accuracy was improved on the CIFAR-10, compared to the generic dataset [$^{ref}$](https://arxiv.org/abs/1801.02929). In a similar fashion, state-of-the-art results on the CIFAR-10 and ImageNet have been achieved using translation, rotation, or shearing of different magnitudes [$^{ref}$](https://arxiv.org/abs/1805.09501). Another method to provide more training samples was to cut the \"main\" component of the image, and paste it on different backgrounds [$^{ref}$](http://openaccess.thecvf.com/content_iccv_2017/html/Dwibedi_Cut_Paste_and_ICCV_2017_paper.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous methods add information to the classifier by performing simple transformation, which creates new and unseen feature combinations. Other possible methods to generate more training samples include generative adversarial networks (GAN) and variational auto-encoders (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 GAN \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are neural networks defined by a _generator_ and a _discriminator_. The former generates increasingly realistic samples, and the discriminator determines if the samples looks \"real\" or \"fake\". The term adversarial refers to the competitive nature of the interaction between generation and discrimination. GANs will be be explained in more details later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 How GAN has been used to counter class imbalance \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers have used GANs to generate new minority samples [$^{ref}$](https://arxiv.org/abs/1807.04585). A balanced GAN (BAGAN) was designed with both the majority and minority class to learn useful features. The authors found that the pictures generated were of higher quality than simply using the minority class [$^{ref}$](https://arxiv.org/abs/1803.09655). However, the authors did not test if this resulted in a more accurate classifier. Similar to our research question, researchers have also used GANs to generate instances of multiple classes, and found increased CNN accuracy [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0925231219309257?dgcid=rss_sd_all), over and above generic oversampling. Importantly, intra-class heterogeneity must be captured by the GAN, to provide new boundaries to the parameter space. With this concern in mind, Huang and colleagues (2019) improved classification accuracy with their actor-critic GAN (AC-GAN) [$^{ref}$](https://ieeexplore.ieee.org/document/8784774)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Our experiment {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Our task \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that some encouraging preliminary results have been found to rebalance classes using GANs, we wanted to learn more about different GAN architectures, as well as evaluate their relative performance on a simple classification task.\n",
    "<br>\n",
    "With the help of GANs and VAEs, we will generate a multitude of samples from a minority class, and determine if these generated samples improve a classifier. A simple face recognition task will be assessed: to determine the sex of the person, given an image. \n",
    "It is important to note that our test set will be balanced. This is to ensure that we can evaluate the efficiency of the generating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Our data \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used will be the UTK Face Dataset [$^{ref}$](http://aicip.eecs.utk.edu/wiki/UTKFace), which consists of over 20,000 face images with annotations of age, gender, and ethnicity. Only the pictures labeled as between 10 and 99 years old will be included. 8,000 samples will be kept for both the male and female categories. Next, the female class will be artificially reduced to 10% of its original size, in order to weaken the classifier. The picture size is 60x60 in grayscale.\n",
    "<br>\n",
    "We will be using the minority class ( _female_ ) examples to generate images until it matches the number of examples in the majority class, giving us balanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Our benchmarks \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using two different benchmarks to evaluate the performance of our different models.\n",
    "1. Training classifier on the original unbalanced dataset.\n",
    "2. Simple oversampling minority class. \n",
    "\n",
    "We expect the first benchmark to have the worse performance on the test set, since it has a very different distribution than the balanced data which we will test on.\n",
    "We expect our second benchmark to have a higher performance, for the simple fact that it is not balanced, even through it has duplicated examples.\n",
    "\n",
    "We choose to use simple benchmarks, but we could also have transformed the oversampled examples by rotating, adding blurs, or randomly removing (whitening) pixels, so that we did not have exact duplicates of images in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Our metrics / CNN Classifier \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier will be a convolutional neural network (CNN). CNNs are neural networks which are highly effective in modeling high dimensional data. Since their inception, they have been achieving the best performances on most benchmark image recognition tasks. \n",
    "\n",
    "CNN typically have least one convolutional layer, which serves as a feature detector, by learning kernel features. Each convolutional layer is often followed by a pooling layer, which further reduces the dimension of the data, by taking the maximum of adjacent features for example (2D max pooling).\n",
    "\n",
    "After a series of convolutional and pooling layers, the last pooling layer is typically flattened and then fully connected layers are added until the output layer.\n",
    "\n",
    "CNN are trained using backpropagation, similarly to regular fully connected neural networks.\n",
    "\n",
    "A representation of our network can be found in the <a href='#annex1'>annex</a> section. It is not shown on the image, but we used a relu activativation and softmax for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said previously, the classifier will be trained on balanced classes (except for the lower bound benchmark, where we will leave the classes unbalanced). We will be using the same network for each generator model, using the same early stopping criteria as well as a learning rate of 0.001.\n",
    "Models will be evaluated using accuracy since our testing set is balanced and most of our testing sets will be as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Our models \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, five models will be used to generate samples. All models contain interesting particularities for the task at hand, which are promising in unique ways. Generator models will be trained on the rare class, _females_ in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Variational Auto-Encoder \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational auto-encoder is an auto-encoder which is used mostly to generate data. It's structure is similar to regular auto-encoders, where we try to find a lower dimensional representation of the data by creating a bottleneck in the middle of the neural networks (encoder and decoder).\n",
    "The reason we can't generate examples from regular auto encoder is that we don't know the distribution of the hidden bottleneck layer, meaning that if we were to pass random values through the decoder, we would most likely get a reconstruction that would look nothing like our other examples. It would most likely be a random collection of pixels.\n",
    "\n",
    "In order to know what kind of inputs passed to the decoder will reproduce examples which look like the population examples, we need to make changes to the hidden bottleneck layer. We need to change it so that we can sample from a distribution (vector of means and standard deviations) in the hidden layer, rather than inputing random fixed values to the decoder.\n",
    "In order to efficiently train the auto-encoder with backpropagation, slight changes need to be made to the hidden bottleneck, since backpropagation can't be performed on a distribution.\n",
    "In simple terms, we will be using the reparameterizing trick, which changes the latent layer from a random node to a deterministic node. This is done by adding an third node in the layer before the latent layer, now having the mean node, the standard deviation node, and the new random epsilon node, with $\\epsilon \\sim N(0,1)$. The epsilon node is now the only random part this layer and the backpropagation will only pass through the other two nodes ($\\mu$ and $\\sigma$). The following <a href='#annex2'>image</a> shows the idea behind this transformation.\n",
    "\n",
    "![VAE training](report_img/vae.png)\n",
    "\n",
    "The loss function of VAE will typically be comprised of two parts. The first is a reconstruction loss, similar to regular auto-encoders. The second part in the loss function is a KL divergence term. The KL divergence measures how far our latent distribution is from a gaussian distribution. The reason we want the distribution to be as close to a gaussian as possible is that when we sample points to generate new images, we will sampling from a multi-dimensional gaussian. If the latent distribution is far from gaussian, we will be sampling points in incorrect proportions.\n",
    "\n",
    "Once the model is trained, we will have learned the latent distribution of the bottleneck layer and we can sample points from this distribution. Sampling a point from the latent distribution, and passing it trough the decoder, we can generate a new unique example. This new example will be based on other examples which were used to train the network, but should still be unique, since we are sampling from a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Generative adversarial networks (GAN) \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are comprised of two neural networks, namely a _generator_ and a _discriminator_.\n",
    "\n",
    "The goal of the former is to generate increasingly realistic samples, while the latter, the discriminator's goal is to determine if the samples are \"real\" or \"fake\". The term adversarial refers to the competitive nature of the interaction between the generator and discriminator. \n",
    "\n",
    "Here is how this works in more details.\n",
    "As we said GANs are composed of 2 competing neural networks. \n",
    "The first part is the discriminator, which is a neural network trained to differentiate between real and fake examples, images in our case. The way it is trained at first is to feed it real examples, as well as random noise. In later phases, the discriminator is trained on examples that were generated by the generator, as well as real examples from the original dataset.\n",
    "\n",
    "The Generator's goal is to generate more and more realistic examples. When we talk about realistic examples, we mean that the fake examples generated are classified as real by the discriminator.\n",
    "To generate examples, the network takes in a random point in a latent input space, passes it through the neural network and outputs an image. At first, the network will output random pixels since it doesn't have a good way to map the points in the latent space to face images. At the end of the training, the generator should generate realistic examples.\n",
    "To train the generator, we need to find a way to connect the discriminator's loss function to the generator weights, meaning that when the generator generates a _poor quality image_ , we need to use the information from the discriminator (how it figured out that it was a fake) to update the weights from the generator.\n",
    "\n",
    "This can be done with backpropagation. Since we are training two networks at the same time, and the only network that has a loss function is the discriminator, we need to use it update the weights of both networks at the same time. The following image shows an example representation of a GAN.\n",
    "\n",
    "![gan_copy](gan_copy.jpeg)\n",
    "\n",
    "Both networks are trained at the same time and the training stops when both loss functions stabilize. \n",
    "\n",
    "To review, here is how the network is trained\n",
    "1. Generate one (or few) images from random noise (random point in latent space). Since the generator has not been trained yet, the images will be of poor quality. \n",
    "2. Pass those generated images and the same amount of real images to the discriminator.\n",
    "3. Use the loss function of the generator to update weights for both of the generator and discriminator.\n",
    "4. Repeat steps 1-3 until both network's loss function have converged. \n",
    "\n",
    "The goal here is to have the generator's loss function be very low, while having the discriminator's loss function be high (accuracy at 0.5).\n",
    "\n",
    "We can think of it as a min max game, where the discriminator tries to minimize a loss function d(z), where z is the latent space and where the generator tries to maximize a d(g(z)).\n",
    "\n",
    "In the original GAN paper, the authors used a mixture of rectifier linear activations and sigmoid activations for the discriminator, and maxout activation for the discriminator. They also noted that it is technically possible to use any differentiable activation function.\n",
    "\n",
    "\n",
    "The training steps outlined above seem pretty simple, however, because of the min-max nature of the optimization, many problems can arise. \n",
    "\n",
    "#### Problems with GAN training\n",
    "#### Mode collapse: \n",
    "When we generate new data points, we would like them to be close enough from real examples, but with enough variety so that examples are not exact copies of the training examples, but also that all of the classes are represented, with similar proportions as the training set.\n",
    "The problem of classes being over/under represented in the learned distribution by the generator, versus the real (training) distribution is known as mode collapse. This problem arises because of the fact that the generator is trying to 'trick' the discriminator by exploiting it's weaknesses. What can happen concretely in the case of multi class problems is that the generator generates images from a single class it 'thinks' the discriminator has the biggest difficulty classifying as real/fake. When the discriminator has learned to properly classify those examples from the single class, the generator will move to another class entirely. This process will happen 'forever' and the generator will never learn the proper distribution of the data. \n",
    "Many variations to the GAN try to deal with this problem, notably one which changes the amount of examples provided to the discriminator from 1 to a small amount. The goal of the generator in this case is to predict if the batch of data points is real or fake as a whole. If the generator tries to generate a batch of data points which all come from the same class, the discriminator will be able to figure out that those points do not resemble the real distribution.\n",
    "Changes in the loss functions can also mitigate this problem and we will further discuss them later.\n",
    "\n",
    "#### Diminished gradient: \n",
    "This problem also comes from the fact that we have two competing neural networks. The diminished gradient problem comes in when the discriminator is very good too quickly and it's performance is very high, especially compared to the generator. Since we perform gradient descent based on the discriminator's loss function, when the discriminator's performance is very high, it's loss function is very low and so is it's gradient. When we perform gradient descent to optimize neural networks or other learning algorithms, small gradients usually mean we are close to the optimum and we can stop the optimization. In the case of GANs, this only means that the discriminator is close to it's optimum and says little about the generator, which is what we are trying to use.\n",
    "Since we spread the gradient from the discriminator to the generator to optimize the generator's neural network, when the gradient used for the backpropagation is very small, the weights of the network are left almost unchanged, which means that the generator stops learning.\n",
    "\n",
    "Alternative loss functions can also mitigate this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Softmax GAN \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax GANs [$^{ref}$](https://arxiv.org/abs/1704.06191) are an attempt to fix both of the problems outlined above. The way it does it is by using a softmax cross-entropy loss. The other novel idea in this paper is that a batch of images are passed to the discriminator, instead of only one. This batch is comprised of both real and fake images and the discriminator tries to predict the share (percentage) of images which are real. Both networks are optimized using this softmax loss function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Wasserstein GAN \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein GANs [$^{ref}$](https://arxiv.org/abs/1701.07875) is another attempt to stabilize the learning of both networks. The idea is that instead of assigning a probability of being real or fake, the discriminator scores the realness or fakeness of a given image. The idea here is that we are trying to find (and minimize) the distance between the generated examples and the training examples. The paper discusses multiple distance measures and argues that the Wasserstein distance is the most appropriate to train GANs.\n",
    "One of the advantage of this distance measure is that it is continuous and differentiable almost everywhere. The other advantage they argue is that the gradient does not diminish as the discriminator gets closer to optimality, in fact, it gets more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Deep Convolutional GAN \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGANs [$^{ref}$](https://arxiv.org/abs/1511.06434) are a natural extension to the original GAN proposed a few years before. They leverage the power of convolutional neural networks to improve quality of generated images.\n",
    "Other contributions from this paper come from the fact that features learned from DCGANs are more easily visualizable and interpretable than the features of fully connected neural networks.\n",
    "Another interesting contribution from this paper is the arithmetic properties of the latent vectors, once the GAN has been optimized. This means that if we adding and subtracting latent vectors will create an image that makes sense. For example, if we take a vector which generates a man with glasses and subtract a man without glasses and add a women without glasses, we should get the image of the women with glasses. This means that we can find useful features in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.6 Adversarial Auto-Encoder \\newline\\newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Auto-Encoders [$^{ref}$](https://arxiv.org/abs/1511.05644), which we can think of as a combination of GANs and auto-encoders. The idea is that the discriminator tries to discriminate between examples reconstructed from an autoencoder's latent space, versus a reconstructed image which comes from the distribution which we try to learn.\n",
    "\n",
    "Sample generated <a href='#annex4_8'>images</a> of all five previously mentioned models can be found in annex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Our results  {-}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained previously explained, we have generated enough images to match our majority class. Our previously rare class is now comprised of approximately 10% real examples and 90% generated examples.\n",
    "\n",
    "We will be training the classifier on approximately 12 000 examples (6000 women and 6000 men), except for the simple baseline which has 6 600 examples (600 women and 6000 men).\n",
    "Our testing set will also be balanced, with approximately 10 000 samples. \n",
    "\n",
    "When running this experiment, we found that we had unstable results. By unstable, we mean that our results were highly dependent on the random nature of the artificial down sizing of the majority class, as well as the train test split.\n",
    "\n",
    "We ran the whole experiment 3 times and found vastly different results.\n",
    "Here is the graph of the averaged accuracy of the 3 experiments.\n",
    "\n",
    "![result_graph](report_img/plot.png)\n",
    "\n",
    "We can see that in the averaged performance metrics, none of the methods outperformed oversampling. This being said, in 2 of the 3 experiments, either softmax GAN or VAE outperformed the oversampling method. The only difference between the experiments is the random splits, meaning that our results are quite dependent on how we split our data, and which 10% of the artificial were used to generate images.\n",
    "\n",
    "With this information, we can't confidently conclude that any of the methods tried above significantly outperform the oversampling method.\n",
    "\n",
    "#### How we could have mitigated this problem \\newline\\newline\n",
    "\n",
    "There are many reasons the problem of unstable results could have arisen. Here are some hypothesis:\n",
    "1. We did not do any hyperparameter tuning (we kept mostly default values for learning rates, etc.). This could possibly have stabilized the learning.\n",
    "2. We could have used higher resolution images. This would have allowed our generator images to generate higher quality images and higher quality features, which would have helped the classifiers.\n",
    "3. Different performance metric. We used early stopping, which stopped the learning when the optimization stabilized. We used the test accuracy of the last epoch. It is possible that using an average of the last n epoch's test accuracy instead of only the last epoch's accuracy could have provided more meaningful results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Alternative results (CNN trained on original data, test is generated data) {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our null results, we ought to determine the reason why the classifier did not improve. As can be deduced from the non significant improvement in accuracy, the generated examples most likely did not generate any new information that helped the classifier learn the boundaries between the two classes. In order to test that hypothesis, we designed another experiment. \n",
    "We trained a new adversarial auto-encoder based on all 6 000 women and generated 10 000 female faces. Using the same architecture and hyperparameters, we generated 10 000 male faces, again using 6 000 training samples. We used these 20 000 generated faces as a test sample. \n",
    "Then, we trained a convolutional neural network based on the same male and female original samples, totalling 12 000. We expected to have extremely (if not perfect) accuracy. If the information contained in the generated samples truly is already present in the original pictures, then the test sample would’t be “unseen”.\n",
    "As expected, the model reached a perfect accuracy very quickly. On the second epoch, it reached 100% accuracy on the generated (test) pictures. From that, we conclude that the generated samples are averaged, easy-to-classify replicas of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results presented above do not show without the shadow of a doubt that generating examples from minority classes can't improve classification accuracy. In fact, we did not use most of the state of the art techniques, and we trained all of our models on laptop computers. This can explain the difference between our images and high resolution images generated [$^{ref}$](https://mymodernmet.com/free-ai-generated-faces/) using the current best techniques and more GPU training time. It is possible, yet unknown, that this difference could have affected our results significantly.\n",
    "\n",
    "In fact, in the past weeks, an interesting new paper [$^{ref}$](https://arxiv.org/abs/1911.09665) argues that they were able to achieve significant performance improvement on many of the image recognition task benchmarks using GANs. In fact, they argue that the reason that most experiments similar to our own have failed to capture a performance improvement is that they failed to disentangle the distribution of the generated images and the distribution of the real images when performing batch normalization.\n",
    "\n",
    "This paper is the first (or one of the first) to argue that increased performance can be achieved using GANs and it will be interesting to see if more research confirms those results.\n",
    "\n",
    "This paper does not talk about using this method for unbalanced classes and it would be interesting to see if we could improve on simple techniques like oversampling using their proposed auxiliary batch normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Annex {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>1. CNN architecture\n",
    "\n",
    "![GAN](report_img/cnnimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> 2. Generated images (Variational Auto Encoder)\n",
    "\n",
    "![GAN](report_img/vae_img.jpg)\n",
    "\n",
    "<br> 3. Generated images (Softmax GAN)\n",
    "\n",
    "![GAN](report_img/softmax.png) \n",
    "\n",
    "\n",
    "<br>\\newpage 4. Generated images (Wasserstein GAN)\n",
    "\n",
    "![GAN](report_img/wsgan.png)\n",
    "\n",
    "<br> 5. Generated images (Deep convolutional GAN)\n",
    "\n",
    "![GAN](report_img/dcgan.png)\n",
    "\n",
    "<br>\\newpage 6. Generated images (Adversarial Auto Encoder)\n",
    "\n",
    "![GAN](report_img/aae.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
